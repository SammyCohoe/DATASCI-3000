{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c7f1df4",
   "metadata": {},
   "source": [
    "# Grade: 100 points\n",
    "\n",
    "# Assignment 01: Linear Regression\n",
    "\n",
    "## Instructions\n",
    "\n",
    "#### Follow These Steps before submitting your assignment \n",
    "\n",
    "1. Complete the notebook.\n",
    "\n",
    "2. Make sure all plots have axis labels.\n",
    "\n",
    "3. Once the notebook is complete, `Restart` your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
    "\n",
    "4. Fix any errors until your notebook runs without any problems.\n",
    "\n",
    "5. Submit one completed notebook for the group to OWL by the deadline.\n",
    "\n",
    "6. Please note, a random seed of 42 needs to be set to ensure the reproducability of the results -- *DO NOT* change this random seed. **If you call additional functions that are based on random number generators, you will need to define their seed to 42 as well**. \n",
    "\n",
    "7. Make sure to reference all external code and documentation used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd59110",
   "metadata": {},
   "source": [
    "# Q1 - Data Loading and Exploration\n",
    "\n",
    "`Dataset_Assignment1.csv` lists the soccer players participated in the 2022 FIFA World Cup. Our ultimate goal is to find the best ML model among three candidates that can best predict a player's monetary \"Value\". The dataset has the following attributes:\n",
    "\n",
    "Age: Player age in years\n",
    "Nationality: Players nationality\n",
    "Overall: Player overall performance score (higher better)\n",
    "Potential: Player potential score (higher better)\n",
    "Club: Player home soccer club\n",
    "Value: Player value i.e, the amount of money a club should pay in order to purchase the player (higher better)\n",
    "Wage: Player stipend (higher better)\n",
    "Preferred Foot: Player preferred foot to play\n",
    "International Reputation: Player international fame (higher better)\n",
    "Week Foot: Performance score of player weak foot (higher better)\n",
    "Skill Moves: Player move skill score (higher better)\n",
    "Body Type: Player body type\n",
    "Position: Position player holds on the pitch\n",
    "Height: Player height in CM\n",
    "Weight: Player weight in kg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d192c12c",
   "metadata": {},
   "source": [
    "1. Load the dataset.\n",
    "2. Display basic statistics and inspect for missing data.\n",
    "3. Visualize the distribution of numerical columns.\n",
    "4. **Discussion Question:** Why is it important to explore and visualize the data before building any models? What types of trends or problems could you uncover at this stage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c278db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q1.1 \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "\n",
    "player_data = pd.read_csv('Dataset_Assignment1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5f42c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q1.2\n",
    "print(player_data.head())\n",
    "print(player_data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6d907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q1.3\n",
    "player_data.hist(figsize=(12, 8), bins=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f440c6",
   "metadata": {},
   "source": [
    "**Answer to Q1.4**: It's important to explore and visualize our data because it gives us a better sense of the data we're looking at, we can see errors early, and spot patterns from the right from the jump. We can see a lot of different trends such as missing data, outliers, skewed distributions, and other patterns in the data (linear/non-linear) which can help us in creating a more appropriate model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cda3ed",
   "metadata": {},
   "source": [
    "# Q2 - Feature extraction\n",
    "\n",
    "1. Plot the joint distribution between `Weight` and `Height`.\n",
    "2. The BMI is defined as the body mass divided by the square of the body height, and is expressed in units of kg/m². With this knowledge, see if you can do some meaningful feature extraction and then drop Weight and Height. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8defb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q2.1\n",
    "player_data.plot(x='Weight', y='Height', kind=\"scatter\")\n",
    "plt.title(\"Player Weight vs. Height\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c167e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q2.2\n",
    "# Creating a new column to show player BMI \n",
    "player_data['Height_m'] = player_data['Height'] / 100\n",
    "player_data['BMI'] = player_data['Weight'] / (player_data['Height_m'] ** 2)\n",
    "print(player_data[['Weight', 'Height', 'BMI']].head())\n",
    "player_data = player_data.drop(columns=['Weight', 'Height', 'Height_m'])\n",
    "print(player_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f596e02",
   "metadata": {},
   "source": [
    "# Q3 - Correlation and Feature Selection\n",
    "\n",
    "1. Compute correlation between numerical features.\n",
    "2. Visualize correlations using a heatmap to identify highly correlated features.\n",
    "3. Choose the most correlated feature with the target variable (`Value`) for simple linear regression.\n",
    "4. **Discussion Question:** How do you interpret a correlation value? Does a higher correlation always mean a feature is more important?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27e7a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q3.1\n",
    "# Compute correlation matrix\n",
    "corr_matrix = player_data.corr(numeric_only=True)\n",
    "\n",
    "# Show correlations with the target variable 'Value'\n",
    "print(corr_matrix['Value'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ee0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q3.2\n",
    "plt.matshow(corr_matrix, cmap=\"coolwarm\")\n",
    "plt.xticks(range(len(corr_matrix.columns)), corr_matrix.columns, rotation=90)\n",
    "plt.yticks(range(len(corr_matrix.columns)), corr_matrix.columns)\n",
    "plt.gca().xaxis.set_ticks_position(\"bottom\")\n",
    "plt.title(\"Correlation Heatmap\", pad=20)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf02c994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q3.3\n",
    "target_corr = corr_matrix[\"Value\"].drop(\"Value\")  # exclude self-correlation\n",
    "most_corr_feature = target_corr.abs().idxmax()    # feature with highest absolute correlation\n",
    "print(\"Most correlated feature with Value:\", most_corr_feature)\n",
    "print(\"Correlation:\", target_corr[most_corr_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea694490",
   "metadata": {},
   "source": [
    "**# Answer to Q3.4**: It's my understanding that the closer a correlation is to 1 or -1 the stronger the relationshhip between the variables. A postive correlation means the values are proportional and a negative correlation means they are inversely proportional. A higher correlation doesnt always mean a feature is more important, for example this is only measuring linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7683f65e",
   "metadata": {},
   "source": [
    "# Q4 - Data Normalization & Simple Linear Regression\n",
    "\n",
    "1. Select one highly correlated numerical feature and build a simple linear regression model to predict the target variable.\n",
    "2. Split the data into 80% training and 20% test sets (use the random seed of 42). \n",
    "3. Normalize the dataset.\n",
    "4. Rebuild the simple linear regression model with normalized data.\n",
    "5. Compare the normalized and non-normalized data visually. \n",
    "6. Compare performance of the normalized and non-normalized models visually and using R² and RMSE.\n",
    "7. **Discussion Question:** Why might normalizing data improve model performance? In which situations might normalization not be beneficial?\n",
    "8. Visualize the relationship between the selected feature and the predicted target variable.\n",
    "9. **Discussion Question:** What could cause a simple linear regression model to perform poorly, even if the correlation is high?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b7002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q4.1\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# Select feature (Wage) and target (Value)\n",
    "X = player_data[[\"Wage\"]]   # Predictor\n",
    "y = player_data[\"Value\"]    # Target\n",
    "\n",
    "# Build and train model\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8083770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q4.2\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build and train model\n",
    "modelSplit = LinearRegression()\n",
    "modelSplit.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = modelSplit.predict(X_test)\n",
    "\n",
    "# Performance\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "# Coefficients\n",
    "print(\"Intercept:\", modelSplit.intercept_)\n",
    "print(\"Coefficient (slope):\", modelSplit.coef_[0])\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X_test, y_test, color=\"blue\", label=\"Actual\")\n",
    "plt.plot(X_test, y_pred, color=\"red\", linewidth=2, label=\"Regression line\")\n",
    "plt.xlabel(\"Wage\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Simple Linear Regression with Split\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6e5822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q4.3\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76ed480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q4.4\n",
    "# Rebuild model with normalized data\n",
    "model_norm = LinearRegression()\n",
    "model_norm.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_norm = model_norm.predict(X_test_scaled)\n",
    "\n",
    "# Performance\n",
    "r2_norm = r2_score(y_test, y_pred_norm)\n",
    "rmse_norm = np.sqrt(mean_squared_error(y_test, y_pred_norm))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16551448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q4.5\n",
    "# Scatter plots of actual vs predicted\n",
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Non-normalized\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(X_test, y_test, color=\"blue\", label=\"Actual\")\n",
    "plt.plot(X_test, y_pred, color=\"red\", linewidth=2, label=\"Predicted\")\n",
    "plt.xlabel(\"Wage\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Non-normalized Regression\")\n",
    "plt.legend()\n",
    "\n",
    "# Normalized (plotting with scaled X for regression line)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(X_test_scaled, y_test, color=\"blue\", label=\"Actual (scaled X)\")\n",
    "plt.plot(X_test_scaled, y_pred_norm, color=\"red\", linewidth=2, label=\"Predicted\")\n",
    "plt.xlabel(\"Wage (scaled)\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Normalized Regression\")\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfaa67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q4.6\n",
    "print(\"Non-normalized Model\")\n",
    "print(\"R²:\", r2)\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"\\n\")\n",
    "print(\"\\nNormalized Model\")\n",
    "print(\"R²:\", r2_norm)\n",
    "print(\"RMSE:\", rmse_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8eae1e",
   "metadata": {},
   "source": [
    "**# Answer to Q4.7**: Normalizing that data can help when dealing with multiple features and these features having widely varied scales as the goal of normalizing is to scale the features so no single feature skews the model due to its high magnitude. Normalization is practically useless when dealing with one feature as all we're doing is expressing the predicition line in a different scale. The relationship between the feature and target is the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cf99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q4.8\n",
    "# Coefficients\n",
    "print(\"Intercept:\", model.intercept_)\n",
    "print(\"Coefficient (slope):\", model.coef_[0])\n",
    "\n",
    "# Plot\n",
    "plt.scatter(X_test.values.ravel(), y_test, color=\"blue\", label=\"Actual\")\n",
    "plt.scatter(X_test.values.ravel(), y_pred, color=\"red\", label=\"Predicted\")\n",
    "plt.xlabel(\"Wage\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Wage vs Predicted Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886a2845",
   "metadata": {},
   "source": [
    "**# Answer to Q4.9**: Poor data can make a model perform poorly for example if there were measurment errors or bias. Outliers can also distort the regression line leading to poor predictions for a magority of the data points. Also since we're only using one variable we're failing to put into consideration other features that are influencing the actual values leading to poor predicitions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3689f657",
   "metadata": {},
   "source": [
    "# Q 5 - Multiple Linear Regression\n",
    "\n",
    "1. Select numerical features with correlation above certain thresholds (e.g., 0.2, 0.4, and 0.6).\n",
    "2. Build three different multiple linear regression models using different sets of features based on correlation thresholds.\n",
    "3. Evaluate and compare these models using R² and RMSE.\n",
    "4. **Discussion Question:** How do we decide which features to include in a multiple linear regression model? What challenges might arise from using too many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2591cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q5.1\n",
    "features_A = [\"Value\", \"Potential\", \"Overall\", \"International Reputation\"]\n",
    "features_B = [\"Value\", \"Potential\", \"Overall\"]\n",
    "features_C = [\"Value\"]\n",
    "target = \"Wage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e55c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q5.2\n",
    "def build_and_evaluate(X, y, feature_set, model_name):\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X[feature_set], y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Fit model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "    # Metrics\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "\n",
    "    # Scatter vs Predicted\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5, label=f\"{model_name} Predictions\")\n",
    "    plt.xlabel(\"Actual Wage\")\n",
    "    plt.ylabel(\"Predicted Wage\")\n",
    "    plt.title(f\"{model_name}: Actual vs Predicted\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Print results\n",
    "    print(f\"{model_name} (features={feature_set})\")\n",
    "    print(f\"  R²:   {r2:.4f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\\n\")\n",
    "\n",
    "    return r2, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53acab9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q5.3\n",
    "X = player_data\n",
    "y = player_data[target]\n",
    "\n",
    "r2_0_2, rmse_0_2 = build_and_evaluate(player_data, player_data[\"Value\"], [\"Wage\", \"Potential\", \"Overall\", \"International Reputation\"], \"MLR_model_4features\")\n",
    "r2_0_4, rmse_0_4 = build_and_evaluate(player_data, player_data[\"Value\"], [\"Wage\", \"Potential\", \"Overall\"], \"MLR_model_3features\")\n",
    "r2_0_6, rmse_0_6 = build_and_evaluate(player_data, player_data[\"Value\"], [\"Wage\"], \"MLR_model_1features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ad85d4",
   "metadata": {},
   "source": [
    "**# Answer to Q5.4**: To decide what features to use when using a multiple linear regression model we can look at the features correlation with the target and use our knowledge of the domain. Some challenges that can come up with using too many features are overfitting, causing our model to only really be accurate for its training data and interpretability making it difficult to understand what features are actually influencing the predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c9610b",
   "metadata": {},
   "source": [
    "# Q6 - Model Evaluation and Comparison\n",
    "\n",
    "1. Compare all models (simple and multiple regression models) by printing there R² and RMSE values.\n",
    "2. **Discussion Question:** Which model performs the best and why?\n",
    "3. **Discussion Question:** If a model has a high R² value but a large RMSE, what might that indicate about the model's performance?\n",
    "4. **Discussion Question:** Discuss next steps or potential improvements to the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edfd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer to Q6.1\n",
    "r2_SLR_model_scaled = r2_norm\n",
    "rmse_SLR_model_scaled = rmse\n",
    "print(f\"SLR_model -> R²: {r2_SLR_model_scaled:.4f}, RMSE: {rmse_SLR_model_scaled:.4f}\")\n",
    "print(f\"MLR_model_4features -> R²: {r2_0_2:.4f}, RMSE: {rmse_0_2:.4f}\")\n",
    "print(f\"MLR_model_3features -> R²: {r2_0_4:.4f}, RMSE: {rmse_0_4:.4f}\")\n",
    "print(f\"MLR_model_1features -> R²: {r2_0_6:.4f}, RMSE: {rmse_0_6:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7964b50a",
   "metadata": {},
   "source": [
    "**Answer to Q6.2**: The model that performs best is the model with 4 features, it has the bset fit with a value of 0.72 and the lowest magnitude of error between predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705dc1b",
   "metadata": {},
   "source": [
    "**Answer to Q6.3**: The high R^2 would indicate the model is able to capture the general trend of the relationship but due to the model's high RMSE it would be poor at making precise predictions as they're way off from the actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead441f9",
   "metadata": {},
   "source": [
    "**Answer to Q6.4**: Some next steps and possible improvements to the models could be adding more useful features and testing other types of models besides just linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6316abdb-946c-43f4-ad47-18926d115090",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Q7 - GenAI\n",
    "\n",
    "1. **Discussion Question:** Did you use GenAI and if so, how?\n",
    "2. **Discussion Question:** What limitations did you encounter and how did you overcome them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c406a",
   "metadata": {},
   "source": [
    "**Answer to Q7.1**: I used GenAI when I was unable to figure out the syntax necessary for answering some questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82ab05",
   "metadata": {},
   "source": [
    "**Answer to Q7.2**: Most of the limitations I encounted were relating to syntax and answering discussion questions as it was my first time using most of these libraries and am quite new to ML. I overcame them by referencing the notes and introduction/tutorial files often as well as reading documentation and looking for examples online.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
